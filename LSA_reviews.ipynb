{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "import collections\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_index(\"review_id\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent symantic analysis\n",
    "# it will analyse all reviews and determine all reviews belong to the same concept\n",
    "def LSA(text):\n",
    "    #text is list of reviews of same product\n",
    "    \n",
    "    \n",
    "    # Created TF-IDF Model\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    \n",
    "    # Created SVD(Singular Value Decomposition)\n",
    "    lsa = TruncatedSVD(n_components = 1,n_iter = 100)\n",
    "    lsa.fit(X)\n",
    "    \n",
    "    terms = vectorizer.get_feature_names()\n",
    "    concept_words={}\n",
    "\n",
    "    for j,comp in enumerate(lsa.components_):\n",
    "        componentTerms = zip(terms,comp)\n",
    "        sortedTerms = sorted(componentTerms,key=lambda x:x[1],reverse=True)\n",
    "        sortedTerms = sortedTerms[:10]\n",
    "        concept_words[str(j)] = sortedTerms\n",
    "     \n",
    "    sentence_scores = []\n",
    "    for key in concept_words.keys():\n",
    "        for sentence in text:\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            scores = 0\n",
    "            for word in words:\n",
    "                for word_with_scores in concept_words[key]:\n",
    "                    if word == word_with_scores[0]:\n",
    "                        scores += word_with_scores[1]\n",
    "            sentence_scores.append(scores)\n",
    "    return sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = dataset.groupby(\"product_id\")\n",
    "#grouping dataset by product_id\n",
    "\n",
    "unique_products = dataset[\"product_id\"].unique()\n",
    "#stores list of all product id\n",
    "\n",
    "no_products = len(unique_products.tolist())\n",
    "#no. of products\n",
    "\n",
    "remove_reviews = []\n",
    "#store review_id that are fake \n",
    "    \n",
    "for i in range(no_products):\n",
    "    #iterate through all product reviews \n",
    "    \n",
    "    df = product_df.get_group(unique_products[i])\n",
    "    #dataframe of a single product\n",
    "    \n",
    "    unique_reviews = df.index.tolist()\n",
    "    #list of review_id of reviews of same product\n",
    "    \n",
    "    no_reviews = len(unique_reviews)   \n",
    "    #no. of reviews of same product\n",
    "    \n",
    "    count = no_reviews \n",
    "    #count is no. of reviews that can be analysed\n",
    "    \n",
    "    reviews = []\n",
    "    #list of review texts\n",
    "    \n",
    "    review_id = []\n",
    "    #list of review texts\n",
    "    \n",
    "    for j in range(no_reviews):\n",
    "        #iterate through all reviews\n",
    "        \n",
    "        text = str(df.loc[unique_reviews[j]][\"review_body\"])\n",
    "        # text of a review \n",
    "        \n",
    "        #cleaning the text\n",
    "        text = re.sub(r\"\\W\",\" \",text)             \n",
    "        text = re.sub(r\"\\d\",\" \",text)             \n",
    "        text = re.sub(r\"\\s+[a-z]\\s+\",\" \",text)    \n",
    "        text = re.sub(r\"^[a-z]\\s+\",\" \",text)    \n",
    "        text = re.sub(r\"\\s+[a-z]$\",\" \",text)    \n",
    "        text = re.sub(r\"\\s+\",\" \",text)\n",
    "        \n",
    "        words = nltk.word_tokenize(text)\n",
    "        #text into word list\n",
    "        \n",
    "        if(len(words)==1):\n",
    "        #if only one word in text review\n",
    "            \n",
    "            if(len(text) <=1 or not wordnet.synsets(text) ):\n",
    "            #if word is having only one character or invalid english word\n",
    "                \n",
    "                remove_reviews.append(unique_reviews[j])\n",
    "                #append this review as fake\n",
    "                \n",
    "                count-=1\n",
    "                #review left to be analysed will be decrease by 1\n",
    "                \n",
    "                continue\n",
    "                #check for next review\n",
    "                '''\n",
    "            elif(len(words[0])<=1):\n",
    "                remove_reviews.append(unique_reviews[j])\n",
    "                count-=1\n",
    "                continue\n",
    "                '''\n",
    "        elif(len(words)==0):\n",
    "        #if no words\n",
    "            \n",
    "            remove_reviews.append(unique_reviews[j])\n",
    "            #append this review as fake\n",
    "            \n",
    "            count-=1\n",
    "            #review left to be analysed will be decrease by 1\n",
    "            \n",
    "            continue\n",
    "            #check for next review\n",
    "        \n",
    "        review_id.append(unique_reviews[j])\n",
    "        #valid: append review_id to review_id list for analysis\n",
    "        \n",
    "        reviews.append(text)\n",
    "        #valid: append review_body to reviews list for analysis\n",
    "        \n",
    "    ###########################################################################################\n",
    "    if(count<=0):\n",
    "        #if no reviews left to analyse \n",
    "\n",
    "        continue\n",
    "        #check for next\n",
    "        \n",
    "    if(count==1): \n",
    "        #if only one review is left to analyse\n",
    "        \n",
    "        #check concept between product title and the review\n",
    "        \n",
    "        text = [text,str(df.loc[review_id[0]][\"product_title\"])] \n",
    "        #add product_title and review to the list \n",
    "        \n",
    "        sentence_scores = LSA(text) \n",
    "        #call LSA method to get the score\n",
    "        \n",
    "        if(sentence_scores[0]==0): \n",
    "        #if review score is 0, then it's fake\n",
    "            remove_reviews.append(review_id[0])\n",
    "        continue\n",
    "    \n",
    "    #list of scores of reviews of same product\n",
    "    sentence_scores = LSA(reviews)\n",
    "            \n",
    "    for j in range(len(sentence_scores)):\n",
    "        #iterating through all the scores\n",
    "        \n",
    "        if(sentence_scores[j]==0.00):\n",
    "            # if score is 0, it's fake\n",
    "            remove_reviews.append(review_id[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
